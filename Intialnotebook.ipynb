{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomForestClassifier, IsolationForest\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m classification_report, confusion_matrix, roc_auc_score\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mxgboost\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mxgb\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimblearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mover_sampling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SMOTE\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "# REFUND FRAUD DETECTION SYSTEM\n",
    "# Traditional ML-Based Approach Implementation\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier, IsolationForest\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "import xgboost as xgb\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import datetime\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "\n",
    "class RefundFraudDetectionSystem:\n",
    "    \"\"\"\n",
    "    A comprehensive system for detecting potentially fraudulent refund claims\n",
    "    using traditional machine learning approaches.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Initialize model components\n",
    "        self.preprocessing_pipeline = None\n",
    "        self.classifier = None\n",
    "        self.anomaly_detector = None\n",
    "        self.text_vectorizer = None\n",
    "        self.feature_names = None\n",
    "        self.threshold = 0.7  # Default threshold, can be tuned\n",
    "        \n",
    "    def preprocess_data(self, data):\n",
    "        \"\"\"\n",
    "        Preprocess the raw data, including feature engineering\n",
    "        \n",
    "        Parameters:\n",
    "        data (DataFrame): Raw data containing order and claim information\n",
    "        \n",
    "        Returns:\n",
    "        DataFrame: Processed data with engineered features\n",
    "        \"\"\"\n",
    "        df = data.copy()\n",
    "        \n",
    "        # Temporal features\n",
    "        df['claim_hour'] = df['claim_timestamp'].dt.hour\n",
    "        df['claim_day'] = df['claim_timestamp'].dt.dayofweek\n",
    "        df['claim_weekend'] = df['claim_day'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "        df['days_since_order'] = (df['claim_timestamp'] - df['order_timestamp']).dt.days\n",
    "        df['days_since_delivery'] = (df['claim_timestamp'] - df['delivery_timestamp']).dt.days\n",
    "        \n",
    "        # If close to expiry date for applicable products\n",
    "        if 'expiry_date' in df.columns:\n",
    "            df['days_to_expiry'] = (df['expiry_date'] - df['claim_timestamp']).dt.days\n",
    "            df['near_expiry'] = df['days_to_expiry'].apply(lambda x: 1 if x < 7 else 0)\n",
    "        \n",
    "        # Customer history features\n",
    "        customer_stats = df.groupby('customer_id').agg({\n",
    "            'order_id': 'count',\n",
    "            'claim_id': 'count',\n",
    "            'total_order_value': 'sum',\n",
    "            'refund_amount': 'sum'\n",
    "        }).reset_index()\n",
    "        \n",
    "        customer_stats.columns = ['customer_id', 'total_orders', 'total_claims', \n",
    "                                 'lifetime_order_value', 'lifetime_refund_value']\n",
    "        \n",
    "        customer_stats['claim_rate'] = customer_stats['total_claims'] / customer_stats['total_orders']\n",
    "        customer_stats['refund_to_order_ratio'] = customer_stats['lifetime_refund_value'] / customer_stats['lifetime_order_value']\n",
    "        \n",
    "        # Merge customer stats back to main dataframe\n",
    "        df = pd.merge(df, customer_stats, on='customer_id', how='left')\n",
    "        \n",
    "        # Product category features\n",
    "        category_claim_rate = df.groupby('product_category').agg({\n",
    "            'claim_id': 'count',\n",
    "            'order_id': 'count'\n",
    "        }).reset_index()\n",
    "        \n",
    "        category_claim_rate['category_claim_rate'] = category_claim_rate['claim_id'] / category_claim_rate['order_id']\n",
    "        df = pd.merge(df, category_claim_rate[['product_category', 'category_claim_rate']], \n",
    "                     on='product_category', how='left')\n",
    "        \n",
    "        # Recent claim pattern\n",
    "        recent_claims = df.sort_values('claim_timestamp').groupby('customer_id').agg({\n",
    "            'claim_timestamp': lambda x: list(x)[-3:] if len(list(x)) >= 3 else list(x),\n",
    "            'claim_reason': lambda x: list(x)[-3:] if len(list(x)) >= 3 else list(x)\n",
    "        }).reset_index()\n",
    "        \n",
    "        # Calculate time between recent claims\n",
    "        def calc_time_between_claims(timestamps):\n",
    "            if len(timestamps) < 2:\n",
    "                return [None]\n",
    "            return [(timestamps[i] - timestamps[i-1]).days for i in range(1, len(timestamps))]\n",
    "        \n",
    "        recent_claims['days_between_claims'] = recent_claims['claim_timestamp'].apply(calc_time_between_claims)\n",
    "        \n",
    "        # Check for repeated same reason\n",
    "        def check_repeated_reason(reasons):\n",
    "            if len(reasons) < 2:\n",
    "                return 0\n",
    "            return 1 if len(set(reasons)) == 1 else 0\n",
    "        \n",
    "        recent_claims['repeated_reason'] = recent_claims['claim_reason'].apply(check_repeated_reason)\n",
    "        \n",
    "        # Calculate average time between claims\n",
    "        def avg_time_between_claims(days_list):\n",
    "            days_list = [d for d in days_list if d is not None]\n",
    "            return np.mean(days_list) if days_list else None\n",
    "        \n",
    "        recent_claims['avg_days_between_claims'] = recent_claims['days_between_claims'].apply(avg_time_between_claims)\n",
    "        \n",
    "        # Merge back to main dataframe\n",
    "        df = pd.merge(df, recent_claims[['customer_id', 'repeated_reason', 'avg_days_between_claims']], \n",
    "                     on='customer_id', how='left')\n",
    "        \n",
    "        # Calculate z-score for claim timing pattern\n",
    "        df['claim_timing_zscore'] = (df['avg_days_between_claims'] - df['avg_days_between_claims'].mean()) / df['avg_days_between_claims'].std()\n",
    "        \n",
    "        # Clean and preprocess text\n",
    "        if 'claim_description' in df.columns:\n",
    "            df['cleaned_description'] = df['claim_description'].apply(self._clean_text)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def _clean_text(self, text):\n",
    "        \"\"\"Clean and normalize text data\"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "        \n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove special characters\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        \n",
    "        # Remove numbers\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        \n",
    "        # Remove stopwords\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        words = text.split()\n",
    "        words = [w for w in words if w not in stop_words]\n",
    "        \n",
    "        return ' '.join(words)\n",
    "    \n",
    "    def engineer_features(self, df):\n",
    "        \"\"\"Additional feature engineering before model training\"\"\"\n",
    "        \n",
    "        # Extract categorical and numerical columns\n",
    "        categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "        numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "        \n",
    "        # Remove target column and non-feature columns\n",
    "        exclude_cols = ['fraud_flag', 'customer_id', 'order_id', 'claim_id', 'claim_timestamp', \n",
    "                         'order_timestamp', 'delivery_timestamp', 'claim_description']\n",
    "        \n",
    "        categorical_cols = [col for col in categorical_cols if col not in exclude_cols]\n",
    "        numerical_cols = [col for col in numerical_cols if col not in exclude_cols and 'cleaned_' not in col]\n",
    "        \n",
    "        # Create preprocessing pipeline\n",
    "        categorical_transformer = Pipeline(steps=[\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ])\n",
    "        \n",
    "        numerical_transformer = Pipeline(steps=[\n",
    "            ('scaler', StandardScaler())\n",
    "        ])\n",
    "        \n",
    "        # Combine preprocessors\n",
    "        self.preprocessing_pipeline = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('num', numerical_transformer, numerical_cols),\n",
    "                ('cat', categorical_transformer, categorical_cols)\n",
    "            ])\n",
    "        \n",
    "        # Add text vectorization if text column exists\n",
    "        if 'cleaned_description' in df.columns:\n",
    "            self.text_vectorizer = TfidfVectorizer(max_features=100)\n",
    "            text_features = self.text_vectorizer.fit_transform(df['cleaned_description'])\n",
    "            \n",
    "            # Store feature names for later use\n",
    "            feature_names = (\n",
    "                numerical_cols +\n",
    "                self.text_vectorizer.get_feature_names_out().tolist() +\n",
    "                self.preprocessing_pipeline.named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(categorical_cols).tolist()\n",
    "            )\n",
    "            self.feature_names = feature_names\n",
    "            \n",
    "            return text_features\n",
    "        \n",
    "        # Store feature names for later use\n",
    "        self.feature_names = (\n",
    "            numerical_cols +\n",
    "            self.preprocessing_pipeline.named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(categorical_cols).tolist()\n",
    "        )\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def train_models(self, X_train, y_train, text_features_train=None):\n",
    "        \"\"\"\n",
    "        Train the classification and anomaly detection models\n",
    "        \n",
    "        Parameters:\n",
    "        X_train (DataFrame): Training features\n",
    "        y_train (Series): Training labels\n",
    "        text_features_train (sparse matrix): Text features if available\n",
    "        \n",
    "        Returns:\n",
    "        self: Trained model\n",
    "        \"\"\"\n",
    "        # Apply preprocessing pipeline\n",
    "        X_train_processed = self.preprocessing_pipeline.fit_transform(X_train)\n",
    "        \n",
    "        # Combine with text features if available\n",
    "        if text_features_train is not None:\n",
    "            X_train_processed = np.hstack([X_train_processed.toarray(), text_features_train.toarray()])\n",
    "        \n",
    "        # Handle class imbalance with SMOTE\n",
    "        smote = SMOTE(random_state=42)\n",
    "        X_resampled, y_resampled = smote.fit_resample(X_train_processed, y_train)\n",
    "        \n",
    "        # Train anomaly detector on legitimate claims only\n",
    "        legitimate_indices = np.where(y_train == 0)[0]\n",
    "        X_legitimate = X_train_processed[legitimate_indices]\n",
    "        \n",
    "        self.anomaly_detector = IsolationForest(\n",
    "            contamination=0.05,  # Assuming 5% of \"legitimate\" claims might be anomalous\n",
    "            random_state=42\n",
    "        )\n",
    "        self.anomaly_detector.fit(X_legitimate)\n",
    "        \n",
    "        # Train classifier\n",
    "        self.classifier = xgb.XGBClassifier(\n",
    "            scale_pos_weight=len(y_train) / sum(y_train),  # Additional class weight balance\n",
    "            use_label_encoder=False,\n",
    "            eval_metric='auc',\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        # Grid search for hyperparameter tuning\n",
    "        param_grid = {\n",
    "            'max_depth': [3, 5, 7],\n",
    "            'learning_rate': [0.01, 0.1, 0.2],\n",
    "            'n_estimators': [100, 200]\n",
    "        }\n",
    "        \n",
    "        grid_search = GridSearchCV(\n",
    "            self.classifier,\n",
    "            param_grid,\n",
    "            cv=5,\n",
    "            scoring='roc_auc',\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        grid_search.fit(X_resampled, y_resampled)\n",
    "        self.classifier = grid_search.best_estimator_\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def evaluate_model(self, X_test, y_test, text_features_test=None):\n",
    "        \"\"\"\n",
    "        Evaluate the model performance\n",
    "        \n",
    "        Parameters:\n",
    "        X_test (DataFrame): Test features\n",
    "        y_test (Series): Test labels\n",
    "        text_features_test (sparse matrix): Text features if available\n",
    "        \n",
    "        Returns:\n",
    "        dict: Performance metrics\n",
    "        \"\"\"\n",
    "        # Apply preprocessing pipeline\n",
    "        X_test_processed = self.preprocessing_pipeline.transform(X_test)\n",
    "        \n",
    "        # Combine with text features if available\n",
    "        if text_features_test is not None:\n",
    "            X_test_processed = np.hstack([X_test_processed.toarray(), text_features_test.toarray()])\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred_proba = self.classifier.predict_proba(X_test_processed)[:, 1]\n",
    "        y_pred = (y_pred_proba >= self.threshold).astype(int)\n",
    "        \n",
    "        # Get anomaly scores (-1 for anomalies, 1 for normal)\n",
    "        anomaly_scores = self.anomaly_detector.decision_function(X_test_processed)\n",
    "        \n",
    "        # Combine classifier and anomaly detector\n",
    "        # Flag as fraud if either classifier confidence is high or anomaly score is very low\n",
    "        combined_pred = np.logical_or(\n",
    "            y_pred == 1,\n",
    "            anomaly_scores < -0.5  # Threshold for strong anomalies\n",
    "        ).astype(int)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics = {\n",
    "            'accuracy': (combined_pred == y_test).mean(),\n",
    "            'roc_auc': roc_auc_score(y_test, y_pred_proba),\n",
    "            'classification_report': classification_report(y_test, combined_pred),\n",
    "            'confusion_matrix': confusion_matrix(y_test, combined_pred)\n",
    "        }\n",
    "        \n",
    "        print(f\"ROC AUC Score: {metrics['roc_auc']:.4f}\")\n",
    "        print(f\"Classification Report:\\n{metrics['classification_report']}\")\n",
    "        \n",
    "        # Plot confusion matrix\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(metrics['confusion_matrix'], annot=True, fmt='d', cmap='Blues',\n",
    "                   xticklabels=['Legitimate', 'Fraud'],\n",
    "                   yticklabels=['Legitimate', 'Fraud'])\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('Actual')\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.show()\n",
    "        \n",
    "        # Plot feature importance\n",
    "        self._plot_feature_importance()\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def _plot_feature_importance(self):\n",
    "        \"\"\"Plot feature importance from the XGBoost classifier\"\"\"\n",
    "        if not hasattr(self.classifier, 'feature_importances_'):\n",
    "            return\n",
    "        \n",
    "        # Get feature importance\n",
    "        importances = self.classifier.feature_importances_\n",
    "        \n",
    "        # Sort feature importances in descending order\n",
    "        indices = np.argsort(importances)[::-1]\n",
    "        \n",
    "        # Select top 15 features\n",
    "        top_n = min(15, len(importances))\n",
    "        \n",
    "        # Display the feature ranking\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.title(\"Feature importances\")\n",
    "        plt.bar(range(top_n), importances[indices[:top_n]], align=\"center\")\n",
    "        plt.xticks(range(top_n), np.array(self.feature_names)[indices[:top_n]], rotation=90)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def predict(self, data):\n",
    "        \"\"\"\n",
    "        Make predictions on new data\n",
    "        \n",
    "        Parameters:\n",
    "        data (DataFrame): New data to make predictions on\n",
    "        \n",
    "        Returns:\n",
    "        DataFrame: Original data with fraud probability and flags\n",
    "        \"\"\"\n",
    "        # Preprocess data\n",
    "        processed_data = self.preprocess_data(data)\n",
    "        \n",
    "        # Extract text features if available\n",
    "        text_features = None\n",
    "        if 'cleaned_description' in processed_data.columns and self.text_vectorizer is not None:\n",
    "            text_features = self.text_vectorizer.transform(processed_data['cleaned_description'])\n",
    "        \n",
    "        # Apply preprocessing pipeline\n",
    "        X_processed = self.preprocessing_pipeline.transform(processed_data)\n",
    "        \n",
    "        # Combine with text features if available\n",
    "        if text_features is not None:\n",
    "            X_processed = np.hstack([X_processed.toarray(), text_features.toarray()])\n",
    "        \n",
    "        # Get fraud probability from classifier\n",
    "        fraud_proba = self.classifier.predict_proba(X_processed)[:, 1]\n",
    "        \n",
    "        # Get anomaly scores (-1 for anomalies, 1 for normal)\n",
    "        anomaly_scores = self.anomaly_detector.decision_function(X_processed)\n",
    "        \n",
    "        # Add predictions to original data\n",
    "        result = data.copy()\n",
    "        result['fraud_probability'] = fraud_proba\n",
    "        result['anomaly_score'] = anomaly_scores\n",
    "        \n",
    "        # Flag as fraud if either classifier confidence is high or anomaly score is very low\n",
    "        result['fraud_flag'] = np.logical_or(\n",
    "            fraud_proba >= self.threshold,\n",
    "            anomaly_scores < -0.5  # Threshold for strong anomalies\n",
    "        ).astype(int)\n",
    "        \n",
    "        # Add risk tier\n",
    "        def assign_risk_tier(row):\n",
    "            if row['fraud_probability'] >= 0.8 or row['anomaly_score'] < -0.7:\n",
    "                return 'High Risk'\n",
    "            elif row['fraud_probability'] >= 0.5 or row['anomaly_score'] < -0.3:\n",
    "                return 'Medium Risk'\n",
    "            else:\n",
    "                return 'Low Risk'\n",
    "        \n",
    "        result['risk_tier'] = result.apply(assign_risk_tier, axis=1)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def save_model(self, filepath):\n",
    "        \"\"\"Save the trained model to disk\"\"\"\n",
    "        model_components = {\n",
    "            'preprocessing_pipeline': self.preprocessing_pipeline,\n",
    "            'classifier': self.classifier,\n",
    "            'anomaly_detector': self.anomaly_detector,\n",
    "            'text_vectorizer': self.text_vectorizer,\n",
    "            'feature_names': self.feature_names,\n",
    "            'threshold': self.threshold\n",
    "        }\n",
    "        joblib.dump(model_components, filepath)\n",
    "        print(f\"Model saved to {filepath}\")\n",
    "    \n",
    "    def load_model(self, filepath):\n",
    "        \"\"\"Load a trained model from disk\"\"\"\n",
    "        model_components = joblib.load(filepath)\n",
    "        self.preprocessing_pipeline = model_components['preprocessing_pipeline']\n",
    "        self.classifier = model_components['classifier']\n",
    "        self.anomaly_detector = model_components['anomaly_detector']\n",
    "        self.text_vectorizer = model_components['text_vectorizer']\n",
    "        self.feature_names = model_components['feature_names']\n",
    "        self.threshold = model_components['threshold']\n",
    "        print(f\"Model loaded from {filepath}\")\n",
    "        return self\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate synthetic data for demonstration\n",
    "    def generate_synthetic_data(n_samples=1000, fraud_ratio=0.1):\n",
    "        np.random.seed(42)\n",
    "        \n",
    "        # Generate customer IDs\n",
    "        n_customers = int(n_samples * 0.3)  # Some customers have multiple orders\n",
    "        customer_ids = [f'CUST_{i:05d}' for i in range(n_customers)]\n",
    "        \n",
    "        # Generate order data\n",
    "        data = {\n",
    "            'customer_id': np.random.choice(customer_ids, n_samples),\n",
    "            'order_id': [f'ORDER_{i:06d}' for i in range(n_samples)],\n",
    "            'claim_id': [f'CLAIM_{i:06d}' for i in range(n_samples)],\n",
    "            'product_category': np.random.choice(['Electronics', 'Groceries', 'Clothing', 'Home', 'Beauty'], n_samples),\n",
    "            'total_order_value': np.random.uniform(100, 5000, n_samples),\n",
    "            'refund_amount': np.random.uniform(50, 2000, n_samples),\n",
    "            'claim_reason': np.random.choice(['Expired', 'Damaged', 'Wrong Item', 'Missing Parts', 'Not as Described'], n_samples)\n",
    "        }\n",
    "        \n",
    "        # Generate timestamps\n",
    "        base_date = datetime.datetime(2023, 1, 1)\n",
    "        \n",
    "        # Order timestamps\n",
    "        order_timestamps = [base_date + datetime.timedelta(days=np.random.randint(0, 365)) \n",
    "                           for _ in range(n_samples)]\n",
    "        data['order_timestamp'] = order_timestamps\n",
    "        \n",
    "        # Delivery timestamps (1-5 days after order)\n",
    "        data['delivery_timestamp'] = [ts + datetime.timedelta(days=np.random.randint(1, 6)) \n",
    "                                    for ts in order_timestamps]\n",
    "        \n",
    "        # Claim timestamps (0-30 days after delivery for legitimate, 25-40 for fraudulent)\n",
    "        # Fraudulent claims tend to be made later\n",
    "        fraud_flags = np.random.choice([0, 1], n_samples, p=[1-fraud_ratio, fraud_ratio])\n",
    "        \n",
    "        claim_timestamps = []\n",
    "        for i in range(n_samples):\n",
    "            if fraud_flags[i] == 0:  # Legitimate\n",
    "                delay = np.random.randint(0, 31)\n",
    "            else:  # Fraudulent\n",
    "                delay = np.random.randint(25, 41)\n",
    "            claim_timestamps.append(data['delivery_timestamp'][i] + datetime.timedelta(days=delay))\n",
    "        \n",
    "        data['claim_timestamp'] = claim_timestamps\n",
    "        data['fraud_flag'] = fraud_flags\n",
    "        \n",
    "        # Generate claim descriptions\n",
    "        legitimate_templates = [\n",
    "            \"The product was {issue} when I received it. I would like a refund.\",\n",
    "            \"I received the item but it was {issue}. Please refund.\",\n",
    "            \"Unfortunately the {product} I ordered was {issue} on arrival.\",\n",
    "            \"The {product} doesn't work as expected because it's {issue}.\",\n",
    "            \"I'm disappointed that the {product} came {issue}.\"\n",
    "        ]\n",
    "        \n",
    "        fraudulent_templates = [\n",
    "            \"The product is {issue}. I want my money back immediately.\",\n",
    "            \"Your {product} is completely {issue}. I demand a refund now.\",\n",
    "            \"This is the third time I've received a {issue} {product}. Refund!!!\",\n",
    "            \"I can't believe how {issue} this {product} is. I want a full refund.\",\n",
    "            \"This {product} is the worst I've ever bought, totally {issue}.\"\n",
    "        ]\n",
    "        \n",
    "        issues = ['damaged', 'broken', 'expired', 'defective', 'not as described']\n",
    "        \n",
    "        descriptions = []\n",
    "        for i in range(n_samples):\n",
    "            product = data['product_category'][i].lower()\n",
    "            issue = np.random.choice(issues)\n",
    "            \n",
    "            if fraud_flags[i] == 0:  # Legitimate\n",
    "                template = np.random.choice(legitimate_templates)\n",
    "            else:  # Fraudulent\n",
    "                template = np.random.choice(fraudulent_templates)\n",
    "                \n",
    "            descriptions.append(template.format(product=product, issue=issue))\n",
    "        \n",
    "        data['claim_description'] = descriptions\n",
    "        \n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame(data)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    # Generate synthetic data\n",
    "    df = generate_synthetic_data(5000, 0.15)\n",
    "    \n",
    "    # Split data\n",
    "    train_df, test_df = train_test_split(df, test_size=0.3, random_state=42)\n",
    "    \n",
    "    print(f\"Training data shape: {train_df.shape}\")\n",
    "    print(f\"Testing data shape: {test_df.shape}\")\n",
    "    \n",
    "    # Initialize the fraud detection system\n",
    "    fraud_detector = RefundFraudDetectionSystem()\n",
    "    \n",
    "    # Preprocess data\n",
    "    train_processed = fraud_detector.preprocess_data(train_df)\n",
    "    test_processed = fraud_detector.preprocess_data(test_df)\n",
    "    \n",
    "    # Define features and target\n",
    "    exclude_cols = ['fraud_flag', 'customer_id', 'order_id', 'claim_id', 'claim_timestamp', \n",
    "                   'order_timestamp', 'delivery_timestamp', 'claim_description', 'cleaned_description']\n",
    "    \n",
    "    feature_cols = [col for col in train_processed.columns if col not in exclude_cols]\n",
    "    \n",
    "    X_train = train_processed[feature_cols]\n",
    "    y_train = train_processed['fraud_flag']\n",
    "    \n",
    "    X_test = test_processed[feature_cols]\n",
    "    y_test = test_processed['fraud_flag']\n",
    "    \n",
    "    # Extract text features\n",
    "    text_features_train = fraud_detector.engineer_features(train_processed)\n",
    "    \n",
    "    if 'cleaned_description' in test_processed.columns and fraud_detector.text_vectorizer is not None:\n",
    "        text_features_test = fraud_detector.text_vectorizer.transform(test_processed['cleaned_description'])\n",
    "    else:\n",
    "        text_features_test = None\n",
    "    \n",
    "    # Train models\n",
    "    fraud_detector.train_models(X_train, y_train, text_features_train)\n",
    "    \n",
    "    # Evaluate models\n",
    "    metrics = fraud_detector.evaluate_model(X_test, y_test, text_features_test)\n",
    "    \n",
    "    # Make predictions on new data\n",
    "    predictions = fraud_detector.predict(test_df)\n",
    "    \n",
    "    # View top high-risk cases\n",
    "    high_risk_cases = predictions[predictions['risk_tier'] == 'High Risk'].sort_values('fraud_probability', ascending=False)\n",
    "    print(\"\\nTop 5 high-risk cases:\")\n",
    "    print(high_risk_cases[['customer_id', 'claim_reason', 'fraud_probability', 'anomaly_score', 'risk_tier']].head())\n",
    "    \n",
    "    # Save the model\n",
    "    fraud_detector.save_model(\"refund_fraud_detector.joblib\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
